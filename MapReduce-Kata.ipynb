{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce Kata\n",
    "\n",
    "A code kata is an exercise in programming which helps a programmer hone their skills through practice and repetition. The term was probably first coined by Dave Thomas, co-author of the book \"The Pragmatic Programmer\", in a bow to the Japanese concept of kata in the martial arts.\n",
    "\n",
    "This repository can be found on my GitHub account: https://github.com/matchilling/kata-mapreduce\n",
    "\n",
    "## Basic setup & usage\n",
    "\n",
    "The project uses Docker and GNU make for running mapreduce jobs on AWS EMR. To get started execute the following steps in your terminal.\n",
    "\n",
    "```bash\n",
    "# 1.1 Create an .env file in the project root (c.f. .env.dist)\n",
    "# 1.2 Create an mrjob.conf file in the project root (c.f. mrjob.conf.dist)\n",
    "# 1.3 Build the project image from the Dockerfile\n",
    "$ make build\n",
    "\n",
    "# 2.1 Run bash interactively in a the container\n",
    "$ make shell\n",
    "```\n",
    "\n",
    "Check the self-documented [makefile](https://github.com/matchilling/kata-mapreduce/blob/master/makefile) for further details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram\n",
    "\n",
    "[MRBigram.py](https://github.com/matchilling/kata-mapreduce/blob/master/bin/MRBigram.py) calculates the conditional probability that a word occurs immediately after the word \"my\" using the entire collection of over 200,000 short jokes from ([Kaggle](https://www.kaggle.com/abhinavmoudgil95/short-jokes)).\n",
    "\n",
    "Which 10 words are most likely to be said immediately after the word \"my\", i.e. with the highest conditional probability?\n",
    "\n",
    "```bash\n",
    "# Run MRBigram with a small dataset\n",
    "$ make shell\n",
    "$ ./bin/MRBigram.py data/shortjokes_small.csv\n",
    "\n",
    "# Run MRBigram on AWS EMR with the complete dataset\n",
    "$ make shell\n",
    "$ ./bin/MRBigram.py data/shortjokes_complete.csv -r emr -c mrjob.conf\n",
    "\n",
    "# Download the output from s3\n",
    "$ cat output/MRBigram/result | sort -r | head -n 10\n",
    "# 5.5085675287006115 \"wife\"\n",
    "# 4.0071172642406220 \"girlfriend\"\n",
    "# 2.9517147244497526 \"friend\"\n",
    "# 1.7476296097691764 \"women\"\n",
    "# 1.6208838082238526 \"dad\"\n",
    "# 1.2723328539742120 \"coffee\"\n",
    "# 1.2162722109830110 \"life\"\n",
    "# 1.2089599532015503 \"favorite\"\n",
    "# 1.1212128598240183 \"mom\"\n",
    "# 1.1041509250006094 \"son\"\n",
    "```\n",
    "\n",
    "The complete result can be found here: [output/MRBigram/result](https://github.com/matchilling/kata-mapreduce/blob/master/output/MRBigram/result)\n",
    "\n",
    "![Screenshot AWS EMR web console](https://raw.githubusercontent.com/matchilling/kata-mapreduce/master/output/MRBigram/screenshot_aws_emr_web_gui.png \"Screenshot AWS EMR web console\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mrjob'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a436ef55ba1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#!/usr/bin/env python\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmrjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMRJob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmrjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMRStep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mrjob'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "from pprint import PrettyPrinter\n",
    "\n",
    "pp = lambda x : PrettyPrinter(indent = 2).pprint(x)\n",
    "\n",
    "class MRBigram(MRJob):\n",
    "\n",
    "    SORT_VALUES = True\n",
    "\n",
    "    def combine_counts(self, key, counts):\n",
    "        yield key, sum(counts)\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        def normalize(text):\n",
    "            words = re.compile(r\"[\\w']+\").findall(text)\n",
    "            words = [word.lower() for word in words]\n",
    "            return [word for word in words if not word.isdigit()]\n",
    "\n",
    "        data = line.split(',')\n",
    "\n",
    "        lineNumber = data[0]\n",
    "        text = data[1]\n",
    "        corpus = normalize(\n",
    "            text[1:-1] if (text.startswith('\"') and text.endswith('\"')) else text\n",
    "        )\n",
    "\n",
    "        previous_word = None\n",
    "        for word in corpus:\n",
    "            if previous_word is not None:\n",
    "                yield (previous_word, '##SELF##'), 1\n",
    "                yield (previous_word, word), 1\n",
    "\n",
    "            previous_word = word\n",
    "\n",
    "    def reducer(self, previous_word, value):\n",
    "        occurrence = 0.0\n",
    "\n",
    "        for id, data in value:\n",
    "            if 'total_01' == id:\n",
    "                occurrence = float(data)\n",
    "            elif 'total_02' == id and 'my' == previous_word:\n",
    "                yield (100.0 * float(data[1]) / occurrence, data[0])\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper, combiner=self.combine_counts, reducer=self.summarise_counts),\n",
    "            MRStep(reducer=self.reducer)\n",
    "        ]\n",
    "\n",
    "    def summarise_counts(self, key, counts):\n",
    "        previous_word, word = key\n",
    "        count = sum(counts)\n",
    "\n",
    "        if '##SELF##' == word:\n",
    "            yield previous_word, ('total_01', count)\n",
    "        else:\n",
    "            yield previous_word, ('total_02', (word, count))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRBigram.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
